{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Using a Pipeline for Text Transformation, Classification, and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import plot_roc_curve, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will practice different techniques to use scikit-learn pipelines to automate the machine learning workflow. You will:\n",
    "\n",
    "1. Load the Airbnb \"listings\" data set.\n",
    "2. Combine two text columns to create one feature.\n",
    "3. Use a scikit-learn pipeline to transform text data using a TF-IDF vectorizer and fit a logistic regression model to the transformed data. \n",
    "4. Evaluate the performance.\n",
    "5. Add a grid search to the pipeline to find the optimal hyperparameter configuration.\n",
    "6. Evaluate the performance of the optimal configuration using ROC-AUC.\n",
    "7. Repeat the above steps using decision trees. \n",
    "8. Compare the performances of using a logistic regression model vs. a decision tree for this problem.\n",
    "\n",
    "**<font color='red'>Note: some of the code cells in this notebook may take a while to run</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Prepare the Data\n",
    "\n",
    "We will work with a new version of the familiar Airbnb \"listings\" data set. It contains all of the numerical and binary columns we used previously, but also contains unstructured text fields.\n",
    "\n",
    "### Load the Data Set\n",
    "\n",
    "<b>Task</b>: In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`.\n",
    "\n",
    "You will be working with the file named \"airbnb_text_readytofit.csv.gz\" that is located in a folder named \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"airbnb_text_readytofit.csv.gz\")\n",
    "df = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>neighborhood_overview</th>\n",
       "      <th>host_name</th>\n",
       "      <th>host_location</th>\n",
       "      <th>host_about</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>neighbourhood_group_cleansed_Brooklyn</th>\n",
       "      <th>neighbourhood_group_cleansed_Manhattan</th>\n",
       "      <th>neighbourhood_group_cleansed_Queens</th>\n",
       "      <th>neighbourhood_group_cleansed_Staten Island</th>\n",
       "      <th>room_type_Entire home/apt</th>\n",
       "      <th>room_type_Hotel room</th>\n",
       "      <th>room_type_Private room</th>\n",
       "      <th>room_type_Shared room</th>\n",
       "      <th>has_availability_True</th>\n",
       "      <th>instant_bookable_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>Beautiful, spacious skylit studio in the heart...</td>\n",
       "      <td>Centrally located in the heart of Manhattan ju...</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>A New Yorker since 2000! My passion is creatin...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.591438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whole flr w/private bdrm, bath &amp; kitchen(pls r...</td>\n",
       "      <td>Enjoy 500 s.f. top floor in 1899 brownstone, w...</td>\n",
       "      <td>Just the right mix of urban center and local n...</td>\n",
       "      <td>LisaRoxanne</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>Laid-back Native New Yorker (formerly bi-coast...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-4.744653</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spacious Brooklyn Duplex, Patio + Garden</td>\n",
       "      <td>We welcome you to stay in our lovely 2 br dupl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rebecca</td>\n",
       "      <td>Brooklyn, New York, United States</td>\n",
       "      <td>Rebecca is an artist/designer, and Henoch is i...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.578481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large Furnished Room Near B'way</td>\n",
       "      <td>Please don’t expect the luxury here just a bas...</td>\n",
       "      <td>Theater district, many restaurants around here.</td>\n",
       "      <td>Shunichi</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>I used to work for a financial industry but no...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.060696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cozy Clean Guest Room - Family Apt</td>\n",
       "      <td>Our best guests are seeking a safe, clean, spa...</td>\n",
       "      <td>Our neighborhood is full of restaurants and ca...</td>\n",
       "      <td>MaryEllen</td>\n",
       "      <td>New York, New York, United States</td>\n",
       "      <td>Welcome to family life with my oldest two away...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.578481</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                              Skylit Midtown Castle   \n",
       "1  Whole flr w/private bdrm, bath & kitchen(pls r...   \n",
       "2           Spacious Brooklyn Duplex, Patio + Garden   \n",
       "3                   Large Furnished Room Near B'way　   \n",
       "4                 Cozy Clean Guest Room - Family Apt   \n",
       "\n",
       "                                         description  \\\n",
       "0  Beautiful, spacious skylit studio in the heart...   \n",
       "1  Enjoy 500 s.f. top floor in 1899 brownstone, w...   \n",
       "2  We welcome you to stay in our lovely 2 br dupl...   \n",
       "3  Please don’t expect the luxury here just a bas...   \n",
       "4  Our best guests are seeking a safe, clean, spa...   \n",
       "\n",
       "                               neighborhood_overview    host_name  \\\n",
       "0  Centrally located in the heart of Manhattan ju...     Jennifer   \n",
       "1  Just the right mix of urban center and local n...  LisaRoxanne   \n",
       "2                                                NaN      Rebecca   \n",
       "3    Theater district, many restaurants around here.     Shunichi   \n",
       "4  Our neighborhood is full of restaurants and ca...    MaryEllen   \n",
       "\n",
       "                       host_location  \\\n",
       "0  New York, New York, United States   \n",
       "1  New York, New York, United States   \n",
       "2  Brooklyn, New York, United States   \n",
       "3  New York, New York, United States   \n",
       "4  New York, New York, United States   \n",
       "\n",
       "                                          host_about  host_is_superhost  \\\n",
       "0  A New Yorker since 2000! My passion is creatin...              False   \n",
       "1  Laid-back Native New Yorker (formerly bi-coast...              False   \n",
       "2  Rebecca is an artist/designer, and Henoch is i...              False   \n",
       "3  I used to work for a financial industry but no...              False   \n",
       "4  Welcome to family life with my oldest two away...              False   \n",
       "\n",
       "   host_has_profile_pic  host_identity_verified  host_response_rate  ...  \\\n",
       "0                  True                    True           -0.591438  ...   \n",
       "1                  True                    True           -4.744653  ...   \n",
       "2                  True                    True            0.578481  ...   \n",
       "3                  True                   False           -0.060696  ...   \n",
       "4                  True                    True            0.578481  ...   \n",
       "\n",
       "   neighbourhood_group_cleansed_Brooklyn  \\\n",
       "0                                    0.0   \n",
       "1                                    1.0   \n",
       "2                                    0.0   \n",
       "3                                    0.0   \n",
       "4                                    1.0   \n",
       "\n",
       "   neighbourhood_group_cleansed_Manhattan  \\\n",
       "0                                     1.0   \n",
       "1                                     0.0   \n",
       "2                                     1.0   \n",
       "3                                     1.0   \n",
       "4                                     0.0   \n",
       "\n",
       "   neighbourhood_group_cleansed_Queens  \\\n",
       "0                                  0.0   \n",
       "1                                  0.0   \n",
       "2                                  0.0   \n",
       "3                                  0.0   \n",
       "4                                  0.0   \n",
       "\n",
       "   neighbourhood_group_cleansed_Staten Island  room_type_Entire home/apt  \\\n",
       "0                                         0.0                        1.0   \n",
       "1                                         0.0                        1.0   \n",
       "2                                         0.0                        0.0   \n",
       "3                                         0.0                        0.0   \n",
       "4                                         0.0                        0.0   \n",
       "\n",
       "   room_type_Hotel room  room_type_Private room  room_type_Shared room  \\\n",
       "0                   0.0                     0.0                    0.0   \n",
       "1                   0.0                     0.0                    0.0   \n",
       "2                   0.0                     1.0                    0.0   \n",
       "3                   0.0                     1.0                    0.0   \n",
       "4                   0.0                     1.0                    0.0   \n",
       "\n",
       "   has_availability_True  instant_bookable_True  \n",
       "0                    1.0                    0.0  \n",
       "1                    1.0                    0.0  \n",
       "2                    1.0                    0.0  \n",
       "3                    1.0                    0.0  \n",
       "4                    1.0                    0.0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27388, 56)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again build a classification model to predict whether or not the host is a superhost. The label will be `host_is_superhost`.\n",
    "\n",
    "This time, the features will be `description` and `neighborhood_overview`. We will combine the values of both features into one feature.\n",
    "\n",
    "<b>Task</b>: Create a new column in DataFrame `df` that contains concatenated values in the columns `description` and `neighborhood_overview`. The new column will be named `combined_text`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Beautiful, spacious skylit studio in the heart...\n",
       "1        Enjoy 500 s.f. top floor in 1899 brownstone, w...\n",
       "2                                                      NaN\n",
       "3        Please don’t expect the luxury here just a bas...\n",
       "4        Our best guests are seeking a safe, clean, spa...\n",
       "                               ...                        \n",
       "27383                                                  NaN\n",
       "27384                                                  NaN\n",
       "27385                                                  NaN\n",
       "27386                                                  NaN\n",
       "27387    Private bedroom on its own floor with very lar...\n",
       "Name: combined_text, Length: 27388, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"combined_text\"] = df[\"description\"] + df[\"neighborhood_overview\"]\n",
    "df[\"combined_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the Examples that Contain Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's removing missing values from our data.\n",
    "\n",
    "<b>Task</b>: \n",
    "\n",
    "1. Drop all rows in DataFrame `df` in which the new column `combined_text` contains missing values.\n",
    "2. Drop all rows in DataFrame `df` in which the column `host_is_superhost` contains missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna(subset = ['description','neighborhood_overview'])\n",
    "# df['host_is_superhost'].dropna()\n",
    "# df[\"combined_text\"]\n",
    "# df=df.dropna(subset=['host_is_superhost'])\n",
    "df = df.dropna(subset= ['host_is_superhost'] )\n",
    "df['combined_text'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18130, 57)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.shape\n",
    "df = df.dropna(subset= ['host_is_superhost'] )\n",
    "df['host_is_superhost'].isnull().values.any()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labeled Examples \n",
    "\n",
    "<b>Task</b>: Create labeled examples from DataFrame `df`. \n",
    "In the code cell below carry out the following steps:\n",
    "\n",
    "* Get the `host_is_superhost` column from DataFrame `df` and assign it to the variable `y`. This will be our label.\n",
    "* Get the `combined_text` column from  DataFrame `df` and assign it to the variable `X`. This will be our feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['host_is_superhost'] \n",
    "X = df['combined_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Labeled Examples into Training and Test Sets\n",
    "\n",
    "<b>Task</b>: In the code cell below create training and test sets out of the labeled examples. \n",
    "\n",
    "1. Use scikit-learn's `train_test_split()` function to create the data sets.\n",
    "\n",
    "2. Specify:\n",
    "    * A test set that is 20 percent of the size of the data set.\n",
    "    * A seed value of '1234'. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.20, random_state=1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Set up a TF-IDF + Logistic Regression Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Set up and fit the pipeline with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below you will create a pipeline that will perform TF-IDF vectorization and fits a logistic regression model to the transformed training data and makes predictions on the transformed test data.\n",
    "\n",
    "\n",
    "<b>Task:</b> Follow the steps to complete the code in the cell below:\n",
    "\n",
    "1. Define a list of steps. Name the list 's'. The list should contain two tuples:\n",
    "\n",
    "    a.  Create a tuple that contains two items:\n",
    "        * a name of your choosing to describe your vectorizer \n",
    "        * a TfidfVectorizer object - do not supply arguments when creating the object\n",
    "\n",
    "    b. Create a tuple that contains two items:\n",
    "        * a name of your choosing to describe your classifier\n",
    "        * a LogisticRegression model object - supply the argument max_iter=300 when creating the object\n",
    "    \n",
    "\n",
    "2. Create a `Pipeline` object: assign the list `s` to the parameter `steps`. Name your pipeline `model_pipeline`.\n",
    "\n",
    "\n",
    "3. Fit `model_pipeline` to the training data (`X_train` and `y_train`).\n",
    "\n",
    "\n",
    "4. Call the `predict_proba()` method to use `model_pipeline` to make predictions on the test data (`X_test`). Save the second column to the variable `probability_predictions`. \n",
    "\n",
    " \n",
    "<b>Note</b>: this will take a number of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin ML pipeline...\n",
      "End pipeline\n"
     ]
    }
   ],
   "source": [
    "print('Begin ML pipeline...')\n",
    "\n",
    "# 1. Define a list of steps. \n",
    "\n",
    "s = [\n",
    "    (\"vectorizer\", TfidfVectorizer()),\n",
    "    (\"model\", LogisticRegression(max_iter=300))\n",
    "]\n",
    "\n",
    "\n",
    "# 2. Create a pipeline object\n",
    "\n",
    "model_pipeline = Pipeline(steps=s)\n",
    "\n",
    "\n",
    "# 3. Fit the pipeline to the training data\n",
    "\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 4. Use the predict_proba() method to make predictions on the test data \n",
    "\n",
    "probability_predictions = model_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('End pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Use the `roc_auc_score()` function to compute the AUC. Save the result to the variable `auc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on the test data: 0.6968\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluate the performance by computing the AUC\n",
    "\n",
    "auc = roc_auc_score(y_test, probability_predictions)\n",
    "\n",
    "\n",
    "print('AUC on the test data: {:.4f}'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Perform a grid search cv on the pipeline to find the best hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will perform a grid search on the pipeline object to find the hyperparameter configuration for hyperparameter $C$ (for the logistic regression) and for the $ngram\\_range$ (for the TF-IDF vectorizer) that result in the best cross-validation score.\n",
    "\n",
    "<b>Task:</b> Define a parameter grid to pass to `GridSearchCV()`. Recall that the parameter grid is a dictionary. Name the dictionary `param_grid_LR`.\n",
    "\n",
    "The dictionary should contain two key value pairs:\n",
    "\n",
    "1. a key specifying the  $C$ hyperparamter name, and a value containing the list `[0.1, 1, 10]`.\n",
    "2. a key specifying the $ngram\\_range$ hyperparameter name, and a value containing the list `[(1,1), (1,2)]`.\n",
    "\n",
    "When working with pipelines, the hyperparameter name is the name of the pipeline item (the descriptive name you provided to the item in the pipeline) followed by two underscores, followed by the actual hyperparameter name. For example, if you named your classifier `lrmodel`, the hyperparameter name for $C$ could be `lrmodel__C`. You can find a list containing possible pipeline hyperparameter names you can use by running the code the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vectorizer', 'model', 'vectorizer__analyzer', 'vectorizer__binary', 'vectorizer__decode_error', 'vectorizer__dtype', 'vectorizer__encoding', 'vectorizer__input', 'vectorizer__lowercase', 'vectorizer__max_df', 'vectorizer__max_features', 'vectorizer__min_df', 'vectorizer__ngram_range', 'vectorizer__norm', 'vectorizer__preprocessor', 'vectorizer__smooth_idf', 'vectorizer__stop_words', 'vectorizer__strip_accents', 'vectorizer__sublinear_tf', 'vectorizer__token_pattern', 'vectorizer__tokenizer', 'vectorizer__use_idf', 'vectorizer__vocabulary', 'model__C', 'model__class_weight', 'model__dual', 'model__fit_intercept', 'model__intercept_scaling', 'model__l1_ratio', 'model__max_iter', 'model__multi_class', 'model__n_jobs', 'model__penalty', 'model__random_state', 'model__solver', 'model__tol', 'model__verbose', 'model__warm_start'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': [0.1, 1, 10], 'vectorizer__ngram_range': [(1, 1), (1, 2)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_LR = { \"model__C\" : [0.1, 1, 10], \"vectorizer__ngram_range\" : [(1,1), (1,2)] }\n",
    "\n",
    "param_grid_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Run a grid search on the pipeline.\n",
    "\n",
    "1. Call `GridSearchCV()` with the following arguments:\n",
    "\n",
    "    1. Pipeline object `model_pipeline`.\n",
    "    2. Parameter grid `param_grid_LR`.\n",
    "    3. Specify 3 cross validation folds using the `cv` parameter.\n",
    "    4. Specify that the scoring method is `roc_auc` using the `scoring` parameter.\n",
    "    5. To monitor the progress of the grid search, supply the argument `verbose=2`.\n",
    "    \n",
    "    Assign the output to the object `grid_LR`.\n",
    "    \n",
    "    \n",
    "2. Fit `grid_LR` on the training data (`X_train` and `y_train`) and assign the result to variable `grid_search_LR`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Grid Search...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] model__C=0.1, vectorizer__ngram_range=(1, 1) ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... model__C=0.1, vectorizer__ngram_range=(1, 1), total=   0.7s\n",
      "[CV] model__C=0.1, vectorizer__ngram_range=(1, 1) ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... model__C=0.1, vectorizer__ngram_range=(1, 1), total=   0.6s\n",
      "[CV] model__C=0.1, vectorizer__ngram_range=(1, 1) ....................\n",
      "[CV] ..... model__C=0.1, vectorizer__ngram_range=(1, 1), total=   0.6s\n",
      "[CV] model__C=0.1, vectorizer__ngram_range=(1, 2) ....................\n",
      "[CV] ..... model__C=0.1, vectorizer__ngram_range=(1, 2), total=   1.9s\n",
      "[CV] model__C=0.1, vectorizer__ngram_range=(1, 2) ....................\n",
      "[CV] ..... model__C=0.1, vectorizer__ngram_range=(1, 2), total=   2.2s\n",
      "[CV] model__C=0.1, vectorizer__ngram_range=(1, 2) ....................\n",
      "[CV] ..... model__C=0.1, vectorizer__ngram_range=(1, 2), total=   2.0s\n",
      "[CV] model__C=1, vectorizer__ngram_range=(1, 1) ......................\n",
      "[CV] ....... model__C=1, vectorizer__ngram_range=(1, 1), total=   0.7s\n",
      "[CV] model__C=1, vectorizer__ngram_range=(1, 1) ......................\n",
      "[CV] ....... model__C=1, vectorizer__ngram_range=(1, 1), total=   0.8s\n",
      "[CV] model__C=1, vectorizer__ngram_range=(1, 1) ......................\n",
      "[CV] ....... model__C=1, vectorizer__ngram_range=(1, 1), total=   0.7s\n",
      "[CV] model__C=1, vectorizer__ngram_range=(1, 2) ......................\n",
      "[CV] ....... model__C=1, vectorizer__ngram_range=(1, 2), total=   3.2s\n",
      "[CV] model__C=1, vectorizer__ngram_range=(1, 2) ......................\n",
      "[CV] ....... model__C=1, vectorizer__ngram_range=(1, 2), total=   3.4s\n",
      "[CV] model__C=1, vectorizer__ngram_range=(1, 2) ......................\n",
      "[CV] ....... model__C=1, vectorizer__ngram_range=(1, 2), total=   2.9s\n",
      "[CV] model__C=10, vectorizer__ngram_range=(1, 1) .....................\n",
      "[CV] ...... model__C=10, vectorizer__ngram_range=(1, 1), total=   0.9s\n",
      "[CV] model__C=10, vectorizer__ngram_range=(1, 1) .....................\n",
      "[CV] ...... model__C=10, vectorizer__ngram_range=(1, 1), total=   1.6s\n",
      "[CV] model__C=10, vectorizer__ngram_range=(1, 1) .....................\n",
      "[CV] ...... model__C=10, vectorizer__ngram_range=(1, 1), total=   1.1s\n",
      "[CV] model__C=10, vectorizer__ngram_range=(1, 2) .....................\n",
      "[CV] ...... model__C=10, vectorizer__ngram_range=(1, 2), total=   4.8s\n",
      "[CV] model__C=10, vectorizer__ngram_range=(1, 2) .....................\n",
      "[CV] ...... model__C=10, vectorizer__ngram_range=(1, 2), total=   4.6s\n",
      "[CV] model__C=10, vectorizer__ngram_range=(1, 2) .....................\n",
      "[CV] ...... model__C=10, vectorizer__ngram_range=(1, 2), total=   4.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:   37.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Running Grid Search...')\n",
    "\n",
    "# 1. Run a Grid Search with 3-fold cross-validation and assign the output to the object 'grid_LR'.\n",
    "\n",
    "grid_LR = GridSearchCV(model_pipeline, param_grid_LR, cv= 3, scoring= 'roc_auc', verbose=2)\n",
    "\n",
    "# 2. Fit the model (grid_LR) on the training data and assign the fitted model to the \n",
    "#    variable 'grid_search_LR'\n",
    "\n",
    "grid_search_LR = grid_LR.fit(X_train, y_train)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to see the best pipeline configuration that was determined by the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=10, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=300,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_LR.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Print the best hyperparameters by accessing them by using the `best_params_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('vectorizer',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('model',\n",
      "                 LogisticRegression(C=10, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=300,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l2', random_state=None,\n",
      "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# print('Best value for C: {0}'.format(best_params_))\n",
    "print(grid_search_LR.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the past, after we obtained the best hyperparameter values from a grid search, we re-trained a model with these values in order to evaluate the performance. This time we will do something different. Just as we can pass a Pipeline object directly to `plot_roc_curve()` to evaluate the model, we can pass `grid_search_LR.best_estimator_` to the function `plot_roc_curve()` to evaluate the model. We also pass in the test data (`X_test` and `y_test`). This allows the test data to be passed through the entire pipeline, using the best hyperparameter values.\n",
    "\n",
    "\n",
    "<b>Task</b>: In the code cell below plot the ROC curve and compute the AUC by calling the function `plot_roc_curve()` with the arguments `grid_search_LR.best_estimator_` and the test data (`X_test` and  `y_test`). Note that you can simply just pass `grid_search_LR` to the function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f5994049fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_roc_curve(grid_search_LR, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Set up a TF-IDF + Decision Tree Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Set up and fit the pipeline with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Follow the steps to complete the code in the cell below:\n",
    "\n",
    "1. Define a list of steps. Name the list 's'. The list should contain two tuples:\n",
    "\n",
    "    a.  Create a tuple that contains two items:\n",
    "        * a name of your choosing to describe your vectorizer \n",
    "        * a TfidfVectorizer object - supply the best value for ngram_range that was computed in the grid search\n",
    "\n",
    "    b. Create a tuple that contains two items:\n",
    "        * a name of your choosing to describe your classifier\n",
    "        * a DecisionTreeClassifier model object - do not supply arguments when creating the object\n",
    "    \n",
    "\n",
    "2. Create a `Pipeline` object: assign the list `s` to the parameter `steps`. Name your pipeline `model_DT_pipeline`.\n",
    "\n",
    "\n",
    "3. Fit `model_DT_pipeline` to the training data (`X_train` and `y_train`).\n",
    "\n",
    "\n",
    "4. Call the `predict()` method to use `model_DT_pipeline` to make predictions on the test data (`X_test`). Save the result to a variable named `class_label_predictions`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin ML pipeline...\n",
      "End pipeline\n"
     ]
    }
   ],
   "source": [
    "print('Begin ML pipeline...')\n",
    "\n",
    "# 1. Define a list of steps. \n",
    "\n",
    "s = [\n",
    "    (\"vectorization\", TfidfVectorizer(ngram_range = (1, 2))),\n",
    "    (\"model\", DecisionTreeClassifier())\n",
    "]\n",
    "\n",
    "# 2. Create a the pipeline object\n",
    "\n",
    "model_DT_pipeline = Pipeline(steps = s)\n",
    "\n",
    "\n",
    "# 3. Fit the pipeline to the training data\n",
    "\n",
    "model_DT_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 4. Use the predict() method to make predictions on the test data \n",
    "###########################\n",
    "class_label_predictions = model_DT_pipeline.predict(X_test)\n",
    "\n",
    "print('End pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Use `accuracy_score()` to compute the accuracy of the model. Save the result to the variable `acc_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.6405129619415334\n"
     ]
    }
   ],
   "source": [
    "# 5. Evaluate the model by computing the accuracy score\n",
    "\n",
    "acc_score = accuracy_score(class_label_predictions, y_test)\n",
    "\n",
    "print('Accuracy score: {}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Pipeline` class has the built-in method `score()` that returns the accuracy score for classification models, and the R2 score for regression models.\n",
    "\n",
    "Rather than implementing steps 4 and 5 above, we can use the method `score()` to obtain the accuracy of our model's predictions. Run the code cell below to try this approach. Notice that it returns the same accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6405129619415334"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DT_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Perform a grid search cv on the pipeline to find the best hyperparameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will perform a grid search on the decision tree pipeline object to find the values for hyperparameters $max\\_depth$ and $min\\_samples\\_split$ that result in the best cross-validation score for the decision tree.\n",
    "\n",
    "<b>Task:</b> Define a parameter grid to pass to `GridSearchCV()`. Name the dictionary `param_grid_DT`.\n",
    "\n",
    "The dictionary should contain two key value pairs:\n",
    "\n",
    "1. a key specifying the $max\\_depth$  hyperparameter name, and a value containing the list `[16, 32, 64]`.\n",
    "2. a key specifying the $min\\_samples\\_split$  hyperparameter name, and a value containing the list `[50, 100]`.\n",
    "\n",
    "First, in the code cell below, use the technique demonstrated above to find the hyperparameter names for your dictionary keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vectorization', 'model', 'vectorization__analyzer', 'vectorization__binary', 'vectorization__decode_error', 'vectorization__dtype', 'vectorization__encoding', 'vectorization__input', 'vectorization__lowercase', 'vectorization__max_df', 'vectorization__max_features', 'vectorization__min_df', 'vectorization__ngram_range', 'vectorization__norm', 'vectorization__preprocessor', 'vectorization__smooth_idf', 'vectorization__stop_words', 'vectorization__strip_accents', 'vectorization__sublinear_tf', 'vectorization__token_pattern', 'vectorization__tokenizer', 'vectorization__use_idf', 'vectorization__vocabulary', 'model__ccp_alpha', 'model__class_weight', 'model__criterion', 'model__max_depth', 'model__max_features', 'model__max_leaf_nodes', 'model__min_impurity_decrease', 'model__min_impurity_split', 'model__min_samples_leaf', 'model__min_samples_split', 'model__min_weight_fraction_leaf', 'model__presort', 'model__random_state', 'model__splitter'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DT_pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create the parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_DT = { \"model__max_depth\": [16,32,64], \"model__min_samples_split\": [50,100]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Run a Grid Search on `model_DT_pipeline`, with a 3-fold cross-validation. You will obtain the accuracy score, which is the default scoring method for `GridSearchCV`, so you do not have to specify the scoring method this time. As this will take a while to run, you can specify `verbose=2` to keep track of the progress.\n",
    "\n",
    "\n",
    "Complete the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Grid Search...\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] model__max_depth=16, model__min_samples_split=50 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . model__max_depth=16, model__min_samples_split=50, total=   2.7s\n",
      "[CV] model__max_depth=16, model__min_samples_split=50 ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . model__max_depth=16, model__min_samples_split=50, total=   2.9s\n",
      "[CV] model__max_depth=16, model__min_samples_split=50 ................\n",
      "[CV] . model__max_depth=16, model__min_samples_split=50, total=   2.8s\n",
      "[CV] model__max_depth=16, model__min_samples_split=100 ...............\n",
      "[CV]  model__max_depth=16, model__min_samples_split=100, total=   2.6s\n",
      "[CV] model__max_depth=16, model__min_samples_split=100 ...............\n",
      "[CV]  model__max_depth=16, model__min_samples_split=100, total=   2.6s\n",
      "[CV] model__max_depth=16, model__min_samples_split=100 ...............\n",
      "[CV]  model__max_depth=16, model__min_samples_split=100, total=   2.5s\n",
      "[CV] model__max_depth=32, model__min_samples_split=50 ................\n",
      "[CV] . model__max_depth=32, model__min_samples_split=50, total=   3.7s\n",
      "[CV] model__max_depth=32, model__min_samples_split=50 ................\n",
      "[CV] . model__max_depth=32, model__min_samples_split=50, total=   3.6s\n",
      "[CV] model__max_depth=32, model__min_samples_split=50 ................\n",
      "[CV] . model__max_depth=32, model__min_samples_split=50, total=   3.5s\n",
      "[CV] model__max_depth=32, model__min_samples_split=100 ...............\n",
      "[CV]  model__max_depth=32, model__min_samples_split=100, total=   3.5s\n",
      "[CV] model__max_depth=32, model__min_samples_split=100 ...............\n",
      "[CV]  model__max_depth=32, model__min_samples_split=100, total=   3.6s\n",
      "[CV] model__max_depth=32, model__min_samples_split=100 ...............\n",
      "[CV]  model__max_depth=32, model__min_samples_split=100, total=   3.5s\n",
      "[CV] model__max_depth=64, model__min_samples_split=50 ................\n",
      "[CV] . model__max_depth=64, model__min_samples_split=50, total=   4.9s\n",
      "[CV] model__max_depth=64, model__min_samples_split=50 ................\n",
      "[CV] . model__max_depth=64, model__min_samples_split=50, total=   4.7s\n",
      "[CV] model__max_depth=64, model__min_samples_split=50 ................\n",
      "[CV] . model__max_depth=64, model__min_samples_split=50, total=   5.3s\n",
      "[CV] model__max_depth=64, model__min_samples_split=100 ...............\n",
      "[CV]  model__max_depth=64, model__min_samples_split=100, total=   5.0s\n",
      "[CV] model__max_depth=64, model__min_samples_split=100 ...............\n",
      "[CV]  model__max_depth=64, model__min_samples_split=100, total=   4.4s\n",
      "[CV] model__max_depth=64, model__min_samples_split=100 ...............\n",
      "[CV]  model__max_depth=64, model__min_samples_split=100, total=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Running Grid Search...')\n",
    "\n",
    "\n",
    "# 1. Run a Grid Search with 3-fold cross-validation and assign the output to the object 'grid_DT'.\n",
    "\n",
    "grid_DT = GridSearchCV(model_DT_pipeline,param_grid_DT, cv=3, verbose = 2)\n",
    "\n",
    "\n",
    "\n",
    "# 2. Fit the model (grid_DT) on the training data and assign the fitted model to the \n",
    "#    variable 'grid_search_DT'\n",
    "\n",
    "grid_search_DT = grid_DT.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see the best pipeline configuration. Note that we now see the best values of the decision tree hyperparameters found above, and also see the ngram_range value we specified for the TF-IDF vectorizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('vectorization',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=1.0, max_features=None,\n",
      "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words=None, strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_p...\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('model',\n",
      "                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
      "                                        criterion='gini', max_depth=16,\n",
      "                                        max_features=None, max_leaf_nodes=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=1,\n",
      "                                        min_samples_split=100,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        presort='deprecated', random_state=None,\n",
      "                                        splitter='best'))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search_DT.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Print the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search_DT.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the scikit-learn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for `GridSearchCV`, you will see that it implements a `score()` method based on the scoring method of the underlying Pipeline or model object.\n",
    "\n",
    "The code cell below calls the `score()` method to make a prediction on the test data and output the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6721594043022614"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_DT.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis:</b> Which model performed better, and why do you think that is? How would you go about verifying your intuition?<br> Do you think it was appropriate to concatenate the values of the two text columns we used? Why or why not? What other way could you think of to use multiple text columns for a classification task? Record your findings in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that performed better is the Logistic Regression model because it resulting in a higher accuracy score than the other ML model. We could compare the error formulars in both model then graphically assess their performance against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
