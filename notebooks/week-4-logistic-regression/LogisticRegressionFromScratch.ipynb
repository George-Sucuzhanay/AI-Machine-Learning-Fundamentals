{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Building a Logistic Regression Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will take what you have earned about gradient descent and write a Python class from scratch to train a logistic regression model. You will implement the various mathematical functions learned in the course, such as the gradient and Hessian of the log loss. \n",
    "\n",
    "In the course videos we presented functions that compute the log loss gradient and Hessian and that implement gradient descent for logistic regression. You will do similar work here, only that we'll refactor the code to improve its generality. \n",
    "\n",
    "In this lab, you will complete the following tasks:\n",
    "\n",
    "1. Build a class that can:\n",
    "    1. fit a logistic regression model given training data \n",
    "    2. make predictions on unlabeled examples\n",
    "1. Load the Airbnb \"listings\" data set\n",
    "2. Test whether our class produces the correct estimates on the Airbnb data\n",
    "3. Benchmark our class against `SkLearn`'s logistic regression class for speed and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Logistic Regression Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below contains the logistic regression class that we are building. Your task is to complete the logic within each specified method. Remember, a method is just a function that belongs to that particular class.\n",
    "\n",
    "Below is a breakdown of the methods contained in the class:\n",
    "\n",
    "1. An `__init__()` method that takes in an error tolerance as a stopping criterion, as well as max number of iterations.\n",
    "2. A `predict_proba()` method that takes a given matrix of features X and predicts $p=(1+e^{(-X*W+\\alpha)})^{-1}$ for each entry\n",
    "3. A `compute_gradient()` method that computes the gradient vector $G$\n",
    "4. A `compute_hessian()` method that computes the Hessian. Note that the $H$ can be broken down to the following matrix multiplications: $H=(X^T*Q)\\cdot X$. \n",
    "5. An `update_weights()` method that applies gradient descent to update the weights\n",
    "6. A `check_stop()` method that checks whether the model has converged or the max iterations have been met\n",
    "7. A `fit()` method that trains the model. It takes in the data and runs the gradient optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, tolerance = 10**-8, max_iterations = 20):\n",
    "        \n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights_array = None # holds current weights and intercept (intercept is at the last position)\n",
    "        self.prior_w = None # holds previous weights and intercept (intercept is at the last position)\n",
    "        \n",
    "        # once we are done training, these variables will hold the \n",
    "        # final values for the weights and intercept\n",
    "        self.weights = None\n",
    "        self.intercept = None \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''\n",
    "        Compute probabilities using the inverse logit\n",
    "        - Inputs: The Nx(K+1) matrix with intercept column X\n",
    "        - Outputs: Vector of probabilities of length N\n",
    "        '''\n",
    "                \n",
    "        XW = X.dot(self.weights_array)\n",
    "        P = 1 / (1 + np.exp(-XW))\n",
    "        return P\n",
    "\n",
    "\n",
    "    \n",
    "    def compute_gradient(self, X, Y, P):\n",
    "        '''\n",
    "        Computes the gradient vector\n",
    "        -Inputs:\n",
    "            - The Nx(K+1) matrix with intercept column X\n",
    "            - Nx1 vector y (label) \n",
    "            - Nx1 vector of predictions P\n",
    "        -Outputs: 1x(K+1) vector of gradients\n",
    "        '''\n",
    "        \n",
    "        G = -(Y-P).dot(X) # change#1  \n",
    "        return G\n",
    "    \n",
    "    def compute_hessian(self, X, P):\n",
    "        '''\n",
    "        computes the Hessian matrix\n",
    "        -inputs:\n",
    "            - Nx(K+1) matrix X\n",
    "            - Nx1 vector of predictions P\n",
    "        -outputs:\n",
    "            - KxK Hessian matrix H=X^T * Diag(Q) * X\n",
    "        '''\n",
    "\n",
    "        Q = P*(1-P)\n",
    "        XQ = X.T * Q\n",
    "        H = XQ.dot(X)\n",
    "\n",
    "        return H\n",
    "\n",
    "    \n",
    "    def update_weights(self, X, y):\n",
    "        '''\n",
    "        Updates existing weight vector\n",
    "        -Inputs:\n",
    "            -Nx(Kx1) matrix X\n",
    "            -Nx1 vector y\n",
    "        -Calls predict_proba, compute_gradient and compute_hessian and uses the \n",
    "        return values to update the weights array\n",
    "        '''\n",
    "        \n",
    "        P = self.predict_proba(X)\n",
    "        G = self.compute_gradient(X, y, P)\n",
    "        H = self.compute_hessian(X, P)\n",
    "        self.prior_w = self.weights_array\n",
    "        self.weights_array = self.weights_array - np.linalg.inv(H).dot(G)\n",
    "        \n",
    "      \n",
    "           \n",
    "    def check_stop(self):\n",
    "        '''\n",
    "        check to see if euclidean distance between old and new weights (normalized)\n",
    "        is less than the tolerance\n",
    "        \n",
    "        returns: True or False on whether stopping criteria is met\n",
    "        '''\n",
    "        \n",
    "        w_old_norm = self.prior_w / np.linalg.norm(self.prior_w)\n",
    "        w_new_norm = self.weights_array / np.linalg.norm(self.weights_array)\n",
    "        diff = w_old_norm - w_new_norm\n",
    "        distance = np.sqrt(diff.dot(diff))\n",
    "        stop = None\n",
    "        if distance < self.tolerance:\n",
    "            stop = True\n",
    "        else:\n",
    "            stop = False\n",
    "    \n",
    "        return stop\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X is the Nx(K-1) data matrix\n",
    "        Y is the labels, using {0,1} coding\n",
    "        '''\n",
    "        \n",
    "        #set initial weights - add an extra dimension for the intercept\n",
    "        self.weights_array = np.zeros(X.shape[1] + 1)\n",
    "        \n",
    "        #Initialize the slope parameter to log(base rate/(1-base rate))\n",
    "        self.weights_array[-1] = np.log(y.mean() / (1-y.mean()))\n",
    "        \n",
    "        #create a new X matrix that includes a column of ones for the intercept\n",
    "        X_int = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "\n",
    "        for i in range(self.max_iterations):\n",
    "            self.update_weights(X_int, y)\n",
    "            \n",
    "            # check whether we should\n",
    "            stop = self.check_stop()\n",
    "            if stop:\n",
    "                # since we are stopping, lets save the final weights and intercept\n",
    "                self.set_final_weights()\n",
    "                self.set_final_intercept()\n",
    "                break\n",
    "                \n",
    "    \n",
    "    def set_final_weights(self):\n",
    "        self.weights = self.weights_array[0:-1]\n",
    "        \n",
    "    def set_final_intercept(self):\n",
    "        self.intercept = self.weights_array[-1]  \n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def get_intercept(self):\n",
    "        return self.intercept\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Complete the Class\n",
    "\n",
    "<b>Task</b>: Follow the steps below to complete the code in the `LogisticRegressionScratch` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Step A\n",
    "\n",
    "Complete the `self.predict_proba()` method. This is where we implement the inverse logit. Do the following:\n",
    "1. Create a variable `XW`. Assign it the result of the dot product of the input `X` and `self.weights_array` variable\n",
    "2. Create a variable `P`. Assign it the result of the inverse logit $(1+e^{-XW})^{-1}$\n",
    "3. Make sure the method returns the variable `P` (the `return` statement has been provided for you).\n",
    "\n",
    "\n",
    "#### Step B\n",
    "\n",
    "Complete the `self.compute_gradient()` method. This is where we implement the log loss gradient. Do the following:\n",
    "1. Create a variable `G`. Assign it the result of the gradient computation $-(y-P) \\cdot X$\n",
    "2. Make sure the method returns the variable `G` (the `return` statement has been provided for you).\n",
    "\n",
    "\n",
    "#### Step C\n",
    "\n",
    "Complete the `self.compute_hessian()` method. This is where we implement the log loss Hessian. Do the following:\n",
    "1. Create a variable `Q`. Assign it the result of the following computation $P*(1-P)$\n",
    "2. Create a variable `XQ`. Assign it the result of the following computation $X^T * Q$. Note that $X$ is the input to the method and this is using regular multiplication\n",
    "3. Create a variable called `H`. Assign it the result of the following computation $XQ \\cdot X$. Note that this operation is using the dot product for matrix multiplication\n",
    "4. Make sure the method returns the variable `H` (the `return` statement has been provided for you).\n",
    "\n",
    "\n",
    "#### Step D\n",
    "\n",
    "Complete the `self.update_weights()` method. This is where we implement the gradient descent update. Do the following:\n",
    "1. Create a variable `P`. Call the `self.predict_proba()` method to get predictions and assign the result to variable `P`. Note, when calling a method from within the class you need to call it using `self.predict_proba()`.\n",
    "2. Create a variable `G`. Call the `self.compute_gradient()` method and assign the result to variable `G`.\n",
    "3. Create a variable `H`. Call the `self.compute_hessian()` method to get the Hessian and assign the result to variable `H`.\n",
    "4. Assign the `self.weights_array` variable  to the `self.prior_w` variable. By doing so, the current weight values become the previous weight values.\n",
    "5. Compute the gradient update-step, which is governed by $w_t=w_{t-1}-H^{-1} \\cdot G$, where $w_t$ and $w_{t-1}$ are both the variable `self.weights_array`(You are updating the current weights and therefore want to update the values in `self.weights_array`).  *Hint*: to implement the part $H^{-1} \\cdot G$, use NumPy's `np.linalg.inv()` function and `dot()` method.\n",
    "6. Note: this method does not return any value.\n",
    "\n",
    "\n",
    "#### Step E\n",
    "\n",
    "Complete the `self.check_stop()` method. This is where we implement the stopping criteria. Do the following:\n",
    "1. Create a variable called `w_old_norm`. Normalize `self.prior_w`. You normalize a vector `v` using the following formula $v / \\|v\\|$ where $\\|v\\|$ can be computed using the function `np.linalg.norm(v)`. Assign this result to the variable `w_old_norm`.\n",
    "2. Create a variable called `w_new_norm`. Normalize `self.weights_array` following the same approach. Assign the result to the variable `w_new_norm`. \n",
    "3. Create a variable called `diff` and assign it the value `w_old_norm-w_new_norm`.\n",
    "4. Create a variable called `distance`. Compute $\\sqrt{d \\cdot d}$ where $d$ is the variable `diff` created in the step above. Note that this uses the dot product.\n",
    "5. Create a boolean variable called `stop`. Check whether `distance` is less than `self.tolerance`. If so, assign `True` to the variable `stop`. If not, assign `False` to the variable `stop`.\n",
    "6. Make sure the method returns the variable `stop` (the `return` statement has been provided for you)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Test the Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will test our implementation of logistic regression. Let's first prepare some data that we can use.\n",
    "\n",
    "We will work with the data set `airbnb_readytofit`. This data set already has all the necessary preprocessing steps implemented, including one-hot encoding of the categorical variables, scaling of all numerical variable values, and imputing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"airbnb_readytofit.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Load the data and save it to DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filename, header = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labeled Examples from the Data Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict whether our host is a super host. We will run this only on a subset of features that can help make this prediction. Run the following cell to set our active feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['review_scores_rating','review_scores_cleanliness','review_scores_checkin','review_scores_communication','review_scores_value','host_response_rate','host_acceptance_rate']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Create labeled examples from DataFrame `df`. \n",
    "In the code cell below carry out the following steps:\n",
    "\n",
    "* Get the `host_is_superhost` column from DataFrame `df` and assign it to the variable `y`. This will be our label.\n",
    "* Get the columns in the list `feature_list` from DataFrame `df` and assign them to the variable `X`. These will be our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 28022\n",
      "\n",
      "Number of Features:7\n",
      "['review_scores_rating', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_value', 'host_response_rate', 'host_acceptance_rate']\n"
     ]
    }
   ],
   "source": [
    "y = df['host_is_superhost']\n",
    "X = df[feature_list]\n",
    "print(\"Number of examples: \" + str(X.shape[0]))\n",
    "print(\"\\nNumber of Features:\" + str(X.shape[1]))\n",
    "print(str(list(X.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our labeled examples, let's test out our logistic regression class.\n",
    "\n",
    "<b>Task:</b> Complete the code cell below by writing code to do the following:\n",
    "1. Create an instance of `LogisticRegressionScratch()` using default parameters (i.e. do not supply any arguments). Name this instance `lr`.\n",
    "2. Fit the model `lr` to the training data by calling `lr.fit()` with X and y as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegressionScratch()\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to see the resulting weights and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted weights and intercept are:\n",
      "[ 0.56690005  0.492255    0.201587    0.25551467 -0.00590516  1.71592957\n",
      "  0.26478817] -1.829062262272181\n"
     ]
    }
   ],
   "source": [
    "print('The fitted weights and intercept are:')\n",
    "print(lr.get_weights(), lr.get_intercept())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Compare with Scikit-Learn's Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our logistic regression implementation with the `sklearn` logistic regression implementation. Note that by default scikit-learn uses a different optimization technique. However, our goal is to compare our resulting weights and intercept with those of scikit-learn's implementation, and these should be the same.\n",
    " \n",
    "<b>Task:</b> In the code cell below, write code to does the following:\n",
    "1. Create the scikit-learn `LogisticRegression` model object below and assign to variable `lr_sk`. Use `C=10**10` as the argument to `LogisticRegression()`.\n",
    "\n",
    "2. Fit the model `lr_sk` to the training data by calling `lr_sk.fit()` with X and y as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000000000, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_sk = LogisticRegression(C = 10**10)\n",
    "lr_sk.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to see the resulting weights and intercept. Compare these to our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fitted weights and intercept with sklearn are:\n",
      "[[ 0.56691547  0.49224905  0.20150113  0.25563246 -0.005929    1.71592022\n",
      "   0.26479199]] [-1.82906726]\n"
     ]
    }
   ],
   "source": [
    "print('The fitted weights and intercept with sklearn are:')\n",
    "print(lr_sk.coef_, lr_sk.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the efficiency (or run time) of both methods. We will use the magic function `%timeit` to do this\n",
    "\n",
    "<b>Task:</b> Use the `%timeit` magic function to fit the logistic regression model `lr` on the training data. Hint: use `%timeit` on `lr.fit(X, y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.1 ms ± 14.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use the `%timeit` magic function to fit the logistic regression model `lr_sk` on the training data. Take a look and see which one is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 ms ± 8.33 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit lr_sk.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
