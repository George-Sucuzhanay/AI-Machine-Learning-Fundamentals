{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will train a Logistic Regression model and analyze its performance. You will train the model on \"cell2cell\" -- a telecom company churn prediction data set.\n",
    "\n",
    "Our problem is a binary classification problem. We are trying to predict whether a customer will leave their current telecom company or whether a customer will not leave the company. This problem is well suited for a Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Before you get started, import a few packages. Run the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import the scikit-learn linear model `LogisticRegression` classifier, the `train_test_split()` function for splitting the data into training and test sets, and the metrics `log_loss` and `accuracy_score` to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load a 'ready-to-fit' Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "We will work with a data set called \"cell2celltrain.\" This data set is already pre-processed, with the proper formatting, outliers and missing values taken care of, and all numerical columns scaled to the [0, 1] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"cell2celltrain.csv\")\n",
    "df = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Labeled Examples from the Data Set \n",
    "\n",
    "Let's obtain columns from our data set to create labeled examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Numeric Columns to use as Features\n",
    "\n",
    "To implement a Logistic Regression model, we must use only the numeric columns. \n",
    "\n",
    "\n",
    "In the code cell below: use the Pandas DataFrame <code>select_dtypes()</code> method to obtain all of the column names that have a dtype of \"float64.\" Save the result to a list named `feature_list`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c83bb20720827de690d7e68b5486396e",
     "grade": false,
     "grade_id": "cell-features",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MonthlyRevenue', 'MonthlyMinutes', 'TotalRecurringCharge', 'DirectorAssistedCalls', 'OverageMinutes', 'RoamingCalls', 'PercChangeMinutes', 'PercChangeRevenues', 'DroppedCalls', 'BlockedCalls', 'UnansweredCalls', 'CustomerCareCalls', 'ThreewayCalls', 'ReceivedCalls', 'OutboundCalls', 'InboundCalls', 'PeakCallsInOut', 'OffPeakCallsInOut', 'DroppedBlockedCalls', 'CallForwardingCalls', 'CallWaitingCalls', 'MonthsInService', 'UniqueSubs', 'ActiveSubs', 'Handsets', 'HandsetModels', 'CurrentEquipmentDays', 'AgeHH1', 'AgeHH2', 'RetentionCalls', 'RetentionOffersAccepted', 'ReferralsMadeBySubscriber', 'IncomeGroup', 'AdjustmentsToCreditRating', 'HandsetPrice']\n"
     ]
    }
   ],
   "source": [
    "feature_list = list(df.select_dtypes(include=['float64']).columns)\n",
    "print(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "407a3ad5180072d1f9eb925a0a1fbcf4",
     "grade": true,
     "grade_id": "cell-features-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testFeatureList\n",
    "\n",
    "try:\n",
    "    p, err = testFeatureList(df,feature_list)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y Labeled Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below carry out the following steps:\n",
    "\n",
    "* Get the `Churn` column from DataFrame `df` and assign it to the variable `y`. This will be our label. The label will be either True or False.\n",
    "* Get the columns listed in `feature_list` from DataFrame `df` and assign them to the variable `X`. These will be our features. \n",
    "\n",
    "You should have 51047 labeled examples. Each example contains 35 features and one label (`Churn`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5bbfd2d757cc6a47662581c9217320d0",
     "grade": false,
     "grade_id": "cell-XY",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 51047\n",
      "\n",
      "Number of Features:35\n",
      "['MonthlyRevenue', 'MonthlyMinutes', 'TotalRecurringCharge', 'DirectorAssistedCalls', 'OverageMinutes', 'RoamingCalls', 'PercChangeMinutes', 'PercChangeRevenues', 'DroppedCalls', 'BlockedCalls', 'UnansweredCalls', 'CustomerCareCalls', 'ThreewayCalls', 'ReceivedCalls', 'OutboundCalls', 'InboundCalls', 'PeakCallsInOut', 'OffPeakCallsInOut', 'DroppedBlockedCalls', 'CallForwardingCalls', 'CallWaitingCalls', 'MonthsInService', 'UniqueSubs', 'ActiveSubs', 'Handsets', 'HandsetModels', 'CurrentEquipmentDays', 'AgeHH1', 'AgeHH2', 'RetentionCalls', 'RetentionOffersAccepted', 'ReferralsMadeBySubscriber', 'IncomeGroup', 'AdjustmentsToCreditRating', 'HandsetPrice']\n"
     ]
    }
   ],
   "source": [
    "y = df['Churn']\n",
    "X = df[feature_list]\n",
    "print(\"Number of examples: \" + str(X.shape[0]))\n",
    "print(\"\\nNumber of Features:\" + str(X.shape[1]))\n",
    "print(str(list(X.columns)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "42fe604bd7ea3dc27ae58171e282c375",
     "grade": true,
     "grade_id": "cell-XY-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testXY\n",
    "\n",
    "try:\n",
    "    p, err = testXY(y,X,df)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, use the `train_test_split()` function to create training and test sets out of the labeled examples. \n",
    "\n",
    "You will call `train_test_split()` function with the following arguments:\n",
    "    \n",
    "1. Variable `X` containing features.\n",
    "2. Variable `y` containing the label.\n",
    "3. A test set that is 33% (.33) of the size of the data set.\n",
    "4. A `random_state` seed value of `1234`.\n",
    "    \n",
    "The `train_test_split()` function will return four outputs. Assign these outputs to the following variable names, using the following order:  `X_train`, `X_test`, `y_train`, `y_test`. \n",
    "\n",
    "Note that you will be able to accomplish this using one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9e9e6658863fc8bd10d4689be2798529",
     "grade": false,
     "grade_id": "cell-trainingData",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd79bf35c7d56c3a0d394c8756a6ba40",
     "grade": true,
     "grade_id": "cell-trainingData-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testSplit\n",
    "\n",
    "try:\n",
    "    p, err = testSplit(X_train, X_test, y_train, y_test, df)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the shape property, check the dimensions of the training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34201, 35)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "# X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Fit a Logistic Regression Classification Model and Evaluate the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below contains code that must be completed to train a Logistic Regression classification model, analyze its performance and print the results. The code below will train a Logistic Regression model on the training data, test the resulting model on the test data, and compute and return (1) the log loss of the resulting probability predictions on the test data and (2) the accuracy score of the resulting predicted class labels on the test data.\n",
    "\n",
    "*Note*: It is worth noting that evaluating a model’s training loss and evaluating a model’s accuracy is different. Accuracy measures what fraction of the examples are correctly predicted by the classifier, while training loss measures the average prediction error per training example over all training examples.\n",
    "\n",
    "\n",
    "Your task is to fill in the code to make it work.<br>\n",
    "\n",
    "\n",
    "In the code cell below:\n",
    "\n",
    "\n",
    "1. Use ```LogisticRegression()``` to create a model object, and assign the result to the variable ```model```. You will not provide arguments for its hyperparameters, but will use Scikit-learn's default values.\n",
    "<br>\n",
    "\n",
    "2. Call the ```model.fit()``` method to fit the model to the training data. The first argument should be ```X_train``` and the second argument should be ```y_train```.\n",
    "<br>\n",
    "\n",
    "3. Call the ```model.predict_proba()``` method  with the argument ```X_test``` to use the fitted model to predict values for the test data. Store the outcome in the variable ```probability_predictions```. Note that the `predict_proba()` method returns two columns, one column per class label. The first column contains the probability that an unlabeled example belongs to class `False` (Churn is \"False) and the second column contains the probability that an unlabeled example belongs to class `True` (Churn is \"True\").\n",
    "<br>\n",
    "\n",
    "4. Call the ```log_loss()``` function; the first argument should be `y_test` and the second argument should be `probability_predictions`. Assign the result to variable `l_loss`. Recall that loss indicates how close the prediction probability is to the actual class label. The closer the probability is to the label (for example, a probability of 0.9 that the label is of class \"False\" when the actual class label is indeed \"False\"), the lower the loss.\n",
    "<br>\n",
    "\n",
    "5. Call the ```model.predict()``` method  with the argument ```X_test``` to use the fitted model to predict the class labels for the test data. Store the outcome in the variable ```class_label_predictions```. Note that the `predict()` method returns the class label (True or False) per unlabeled example.\n",
    "<br>\n",
    "\n",
    "6. Call the ```accuracy_score()``` function; the first argument should be `y_test` and the second argument should be `class_label_predictions`. Assign the result to variable `acc_score`. Accuracy refers to the number of class label predictions that are correct.\n",
    "<br>\n",
    "\n",
    "\n",
    "You might find it useful to consult the `LogisticRegression` Scikit-learn online [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to see how to accomplish this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "19ffaae4e364cd63985fe6c1fc30fb00",
     "grade": false,
     "grade_id": "cell-LR",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Prediction Probabilities: \n",
      " Class: False  Class: True\n",
      "     0.745386     0.254614\n",
      "     0.658797     0.341203\n",
      "     0.724719     0.275281\n",
      "     0.848821     0.151179\n",
      "     0.749441     0.250559\n",
      "Log loss: 0.5878612157234154\n",
      "Class labels: [False False False False False]\n",
      "Accuracy: 0.7097827377418972\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "# 1. Create the LogisticRegression model object below and assign to variable 'model'\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# 2. Fit the model to the training data below\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions on the test data using the predict_proba() method and assign the \n",
    "# result to the variable 'probability_predictions' below\n",
    "\n",
    "probability_predictions = model.predict_proba(X_test)\n",
    "\n",
    "# print the first 5 probability class predictions\n",
    "df_print = pd.DataFrame(probability_predictions, columns = ['Class: False', 'Class: True'])\n",
    "print('Class Prediction Probabilities: \\n' + df_print[0:5].to_string(index=False))\n",
    "\n",
    "# 4. Compute the log loss on 'probability_predictions' and save the result to the variable\n",
    "# 'l_loss' below\n",
    "\n",
    "l_loss = log_loss(y_test, probability_predictions)\n",
    "print('Log loss: ' + str(l_loss))\n",
    "\n",
    "\n",
    "# 5. Make predictions on the test data using the predict() method and assign the result \n",
    "# to the variable 'class_label_predictions' below\n",
    "\n",
    "class_label_predictions = model.predict(X_test)\n",
    "\n",
    "# print the first 5 class label predictions \n",
    "print('Class labels: ' + str(class_label_predictions[0:5]))\n",
    "\n",
    "# 6.Compute the accuracy score on 'class_label_predictions' and save the result \n",
    "# to the variable 'acc_score' below\n",
    "\n",
    "acc_score = accuracy_score(y_test,class_label_predictions )\n",
    "print('Accuracy: ' + str(acc_score))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d47071ef1ea9108242579cfb620b9912",
     "grade": true,
     "grade_id": "cell-LR_test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testModel\n",
    "\n",
    "try:\n",
    "    p, err = testModel(probability_predictions, l_loss, class_label_predictions, acc_score, df)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Thresholds: Map Probabilities to a Class Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the output of the code cell above.\n",
    "\n",
    "Note that the `predict_proba()` method returns two columns. As stated, the first column contains the probability that an unlabeled example belongs to class `False` and the second column contains the probability that an unlabeled example belongs to class `True`.\n",
    "\n",
    "The `predict()` method outputs the actual class label (`True` or `False`).\n",
    "\n",
    "In order to determine the order of the classes in the `predict_proba()` output, you can inspect the model object's `classes_` property. Run the cell below and inspect the results. You'll see that indeed, column one gives the probability that an unlabeled example belongs to class `False` and column two gives the probability that an unlabeled example belongs to class `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n"
     ]
    }
   ],
   "source": [
    "print(model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the probabilities map to labels in the table below. The table contains:\n",
    "* 3 unlabeled examples\n",
    "* the resulting class probability values from the logistic regression model's `predict_proba()` method\n",
    "* the corresponding class label from the same logistic regression model's `predict()` method. \n",
    "\n",
    "Inspect \"Example 1\". The probability that that unlabeled \"Example 1\" is of class `False` is 0.745386. The probability that unlabeled \"Example 1\" is of class `True` is 0.254614. The `predict()` method assigns \"Example 1\" the class label `False`.\n",
    "\n",
    "\n",
    "<table align=left>\n",
    "   <tr>\n",
    "    <th></th>\n",
    "    <th>Class: False</th>\n",
    "    <th>Class: True</th>\n",
    "    <th>Class Label</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <th>Example 1</th>\n",
    "    <th>0.745386</th>\n",
    "    <th>0.254614</th>\n",
    "    <th>False</th>\n",
    "    </tr>\n",
    "     <tr>\n",
    "    <th>Example 2</th>\n",
    "    <th>0.745386</th>\n",
    "    <th>0.254614</th>\n",
    "    <th>False</th>\n",
    "    </tr>    \n",
    "     <tr>\n",
    "    <th>Example 3</th>\n",
    "    <th>0.436033</th>\n",
    "    <th>0.563967</th>\n",
    "    <th>True</th>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the Scikit-learn `predict()` method assign a class label based on probability values? For binary classification, the method defaults to a 0.5 threshold. If the resulting probability for class 0 is greater than or equal to 0.5, the unlabeled example is given a label of `False` On the other hand, if the probability for class 0 is less than 0.5,  the unlabeled example is given a label of `True`.\n",
    "\n",
    "Sometimes we may want a different threshold.  Sckit-learn provides different methods to determine the ideal threshold. You will learn about those later in the program. For now, we will use our own approach. Examine the code cell below. It contains the function `computeAccuracy()` that returns the accuracy score of your model using a specified threshold.\n",
    "\n",
    "The function `computeAccuracy()` takes a threshold value as an argument. It does the following:\n",
    "\n",
    "1. Loops through the array `probability_predictions` (obtained from your Logistic Regression model above)\n",
    "    * It extracts the first column's probability \n",
    "    * It checks if that probability is greater than or equal to the threshold value.\n",
    "    * If so, it assigns a class label of `False`. Otherwise it assigns a class label of `True`.\n",
    "    * It saves the new class label to list `labels`.\n",
    "2. Computes the accuracy score by comparing the new class labels contained in list `labels` with the ground truth labels contained in `y_test`.\n",
    "3. Returns the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAccuracy(threshold_value):\n",
    "    \n",
    "    labels=[]\n",
    "    for p in probability_predictions[:,0]:\n",
    "        if p >= threshold_value:\n",
    "            labels.append(False)\n",
    "        else:\n",
    "            labels.append(True)\n",
    "    \n",
    "    acc_score = accuracy_score(y_test, labels)\n",
    "    return acc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below calls the `computeAccuracy()` function with a few different threshold values. Run the cell below and inspect the results. Compare the accuracy of your model with the different threshold values. Which threshold yields the best accuracy score? You can experiment with different threshold values and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold value 0.44: Accuracy 0.7107918793778939\n",
      "Threshold value 0.50: Accuracy 0.7097827377418972\n",
      "Threshold value 0.55: Accuracy 0.7084174284696664\n",
      "Threshold value 0.67: Accuracy 0.6491748783093909\n",
      "Threshold value 0.75: Accuracy 0.49299536982072895\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.44, 0.50, 0.55, 0.67, 0.75]\n",
    "for t in thresholds:\n",
    "    print(\"Threshold value {:.2f}: Accuracy {}\".format(t, str(computeAccuracy(t))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
