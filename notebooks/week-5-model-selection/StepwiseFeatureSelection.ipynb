{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepwise Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement stepwise feature selection.\n",
    "\n",
    "* You will train decision tree models on \"cell2cell,\" a telecom company churn prediction data set.\n",
    "* After using the optimal hyperparameter configuration that results in the best performing decision tree, you will perform feature selection to find the most important features in your training data for predicting customer churn.\n",
    "\n",
    "**<font color='red'>Note: Some of the code cells in this notebook may take a while to run.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "\n",
    "Before you get started, import a few packages. Run the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also import the scikit-learn `DecisionTreeClassifier`, the `train_test_split()` function for splitting the data into training and test sets, and the metric `accuracy_score` to evaluate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load a 'ready-to-fit' Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the \"cell2celltrain\" data set. This data set is already preprocessed, with the proper formatting, outliers and missing values taken care of, and all numerical columns scaled to the [0, 1] interval. One-hot encoding has been performed on all categorical columns. Run the cell below to load the data set and save it to DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), \"data\", \"cell2celltrain.csv\")\n",
    "df = pd.read_csv(filename, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labeled Examples\n",
    "\n",
    "The code cell obtains columns from our data and creates features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>ChildrenInHH</th>\n",
       "      <th>HandsetRefurbished</th>\n",
       "      <th>HandsetWebCapable</th>\n",
       "      <th>TruckOwner</th>\n",
       "      <th>RVOwner</th>\n",
       "      <th>HomeownershipKnown</th>\n",
       "      <th>BuysViaMailOrder</th>\n",
       "      <th>RespondsToMailOffers</th>\n",
       "      <th>OptOutMailings</th>\n",
       "      <th>...</th>\n",
       "      <th>Occupation_Crafts</th>\n",
       "      <th>Occupation_Homemaker</th>\n",
       "      <th>Occupation_Other</th>\n",
       "      <th>Occupation_Professional</th>\n",
       "      <th>Occupation_Retired</th>\n",
       "      <th>Occupation_Self</th>\n",
       "      <th>Occupation_Student</th>\n",
       "      <th>Married_False</th>\n",
       "      <th>Married_True</th>\n",
       "      <th>Married_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000002</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000010</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000014</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000022</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000026</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  ChildrenInHH  HandsetRefurbished  HandsetWebCapable  \\\n",
       "0     3000002         False               False               True   \n",
       "1     3000010          True               False              False   \n",
       "2     3000014          True               False              False   \n",
       "3     3000022         False               False               True   \n",
       "4     3000026         False               False              False   \n",
       "\n",
       "   TruckOwner  RVOwner  HomeownershipKnown  BuysViaMailOrder  \\\n",
       "0       False    False                True              True   \n",
       "1       False    False                True              True   \n",
       "2       False    False               False             False   \n",
       "3       False    False                True              True   \n",
       "4       False    False                True              True   \n",
       "\n",
       "   RespondsToMailOffers  OptOutMailings  ...  Occupation_Crafts  \\\n",
       "0                  True           False  ...                0.0   \n",
       "1                  True           False  ...                0.0   \n",
       "2                 False           False  ...                1.0   \n",
       "3                  True           False  ...                0.0   \n",
       "4                  True           False  ...                0.0   \n",
       "\n",
       "   Occupation_Homemaker  Occupation_Other  Occupation_Professional  \\\n",
       "0                   0.0               0.0                      1.0   \n",
       "1                   0.0               0.0                      1.0   \n",
       "2                   0.0               0.0                      0.0   \n",
       "3                   0.0               1.0                      0.0   \n",
       "4                   0.0               0.0                      1.0   \n",
       "\n",
       "   Occupation_Retired  Occupation_Self  Occupation_Student  Married_False  \\\n",
       "0                 0.0              0.0                 0.0            1.0   \n",
       "1                 0.0              0.0                 0.0            0.0   \n",
       "2                 0.0              0.0                 0.0            0.0   \n",
       "3                 0.0              0.0                 0.0            1.0   \n",
       "4                 0.0              0.0                 0.0            0.0   \n",
       "\n",
       "   Married_True  Married_nan  \n",
       "0           0.0          0.0  \n",
       "1           1.0          0.0  \n",
       "2           1.0          0.0  \n",
       "3           0.0          0.0  \n",
       "4           1.0          0.0  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Churn']\n",
    "X = df.drop(columns = 'Churn', axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Examples Into Training and Test Sets\n",
    "\n",
    "The code cell below creates training and test data sets. Since we will be performing model selection, we will split 10% of our data to serve as a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Perform Decision Tree Model Selection Using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For brevity, let's get back to where we left off in the previous exercise: We already performed a grid search over four different model configurations to find the optimal hyperparameter values for the maximum depth of the tree (`max_depth`) and the minimum number of samples required to be at a leaf node (`min_samples_leaf`). \n",
    "\n",
    "Since we just re-used the same random seed to split our data into the training and test sets, it is safe to set the best hyperparameter values to what we identified in the previous exercise without re-running the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'min_samples_leaf': 50}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not edit this cell\n",
    "best_params = {'max_depth':4, 'min_samples_leaf':50}\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Stepwise Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement our version of stepwise feature selection to find the top performing model. We will continue to use the hyperparameter configuration that we have found to produce an optimal model; we will focus on finding the combination of features that produces the best performing model.\n",
    "\n",
    "We will write a program that fits a decision tree model to the training data then eliminates the least important feature and re-trains the model on a reduced set. We will iterate this process of eliminating and re-training until our stopping criterion is met: The accuracy of the updated model increases by less than 0.1. Once the stopping criterion is met, we will record which features we are left with and consider them the best ones to use. \n",
    "\n",
    "Recall that all model selection is done on *validation data*, not the test set. Therefore, we will use the training data that we created before, and split the training data `X_train` and `y_train` into training and validation sets. We indirectly did this before by using cross-validation functions in `sklearn`. This time, we will perform only one split, and will do so manually. We will perform this split at every iteration of our stepwise feature selection process.\n",
    "\n",
    "To accomplish a stepwise feature selection implementation, we will create a few functions:\n",
    "* `train_evaluate_DT_model()`: to train and evaluate a decision tree model\n",
    "* `feature_to_drop()`: to \"drop\" the lowest scoring feature\n",
    "* `stepwise_feature_selection()`: performs feature selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Function to Train and Evaluate a Decision Tree  \n",
    "\n",
    "<b>Use:</b> The function will be used to train a model with a subset of training data.\n",
    "\n",
    "<b>Arguments:</b>\n",
    "    1. X: features\n",
    "    2. y: label\n",
    "    3. max_depth\n",
    "    4. min_samples_leaf\n",
    "    \n",
    "<b>Returns:</b>\n",
    "    1. the accuracy score\n",
    "    2. the model object\n",
    "    3. a list of the training features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_DT_model(X_train, X_val, y_train, y_val, max_depth, min_samples_leaf):\n",
    "    \n",
    "    #1. Create a DecisionTreeClassifier model object with the best hyperparam values\n",
    "    model = DecisionTreeClassifier(max_depth = max_depth, \n",
    "                                   min_samples_leaf = min_samples_leaf)\n",
    "    \n",
    "    #2. Fit the model to the training data \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #3. Make predictions on the validation data\n",
    "    class_label_predictions = model.predict(X_val)\n",
    "\n",
    "    #4. Compute the accuracy \n",
    "    acc_score = accuracy_score(y_val, class_label_predictions)\n",
    "    \n",
    "    #. Return the accuracy score and the model object\n",
    "    return acc_score, model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Function to Return the Worst Scoring Feature \n",
    "\n",
    "<b>Use:</b> The function will use the feature importance scores from the `model` input to find the name of the feature that has the lowest score. \n",
    "\n",
    "<b>Arguments:</b>\n",
    "   1. decision tree `model` object\n",
    "   2. `numpy` array of feature names\n",
    "    \n",
    "<b>Returns:</b>\n",
    "   1. the name of the lowest scoring feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, you will find the least important feature in your training data for predicting churn.\n",
    "\n",
    "Perform the following steps:\n",
    "\n",
    "1. Using `model.feature_importances_`, obtain scores corresponding to the importance of the predictive features. Save the result to the variable `feature_imp`.\n",
    "\n",
    "2. Create a Pandas DataFrame out of all feature names and their measures of importance by using the `pd.DataFrame()` function.  Call the function with a dictionary containing the following key/value pairs:\n",
    "    * `'name': feature_names`\n",
    "    * `'imp': feature_imp`\n",
    "    \n",
    "    Assign the DataFrame to the variable `df_features`.\n",
    "\n",
    "3. Using the Pandas method `sort_values()`, sort the importance scores in the `imp` column in the new  DataFrame `df_features` in descending order. Assign the resulting DataFrame to variable `df_sorted`.\n",
    "\n",
    "4. Using `iloc`, extract the last value in `df_sorted`. Then extract the value in the `name` column and save it to the variable `lowest_scoring_feature`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graded Cell\n",
    "\n",
    "The cell below will be graded. Remove the line \"raise NotImplementedError()\" before writing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e8a0602fb71fd1973d199d711cfe2c7e",
     "grade": false,
     "grade_id": "cell-lowestscore",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feature_to_drop(model, feature_names):\n",
    "    \n",
    "    #1. Obtain \"feature importance\" scores from the model object and save the list to the \n",
    "    # variable 'feature_imp'\n",
    "    # YOUR CODE HERE\n",
    "    feature_imp = model.feature_importances_\n",
    "\n",
    "    #2. Create a Pandas DataFrame with a list of all features and their scores. \n",
    "    # Save the result to the variable 'df_features'\n",
    "    # YOUR CODE HERE\n",
    "    df_features = pd.DataFrame({'name': feature_names, 'imp': feature_imp })\n",
    "\n",
    "    #3. Sort df_features in descending order and\n",
    "    # save the result to the variable 'df_sorted'\n",
    "    # YOUR CODE HERE\n",
    "    df_sorted = df_features.sort_values(by=['imp'], ascending = False)\n",
    "\n",
    "    #4. Obtain the last feature name and save the result to variable 'lowest_scoring_feature'\n",
    "    # YOUR CODE HERE\n",
    "    lowest_scoring_feature = df_sorted.iloc [-1]['name']\n",
    "    \n",
    "    \n",
    "    return lowest_scoring_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Check\n",
    "\n",
    "Run the cell below to test the correctness of your code above before submitting for grading. Do not add code or delete code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "effed7811a6900035d0c7742789320bb",
     "grade": true,
     "grade_id": "cell-lowestscore-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Run this self-test cell to check your code; \n",
    "# do not add code or delete code in this cell\n",
    "from jn import testHFS\n",
    "\n",
    "try:\n",
    "    p, err = testHFS(df, feature_to_drop)\n",
    "    print(err)\n",
    "except Exception as e:\n",
    "    print(\"Error!\\n\" + str(e))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Function to Perform Stepwise Feature Selection\n",
    "\n",
    "<b>Use:</b> The function will iteratively remove the least important features from the training set, until the accuracy no longer improves by more than 1%. The function uses the two functions implemented above to accomplish this.\n",
    "    \n",
    "<b>Arguments:</b>\n",
    "   1. X_train: features\n",
    "   2. y_train: label\n",
    "   3. max_depth\n",
    "   4. min_samples_leaf\n",
    "    \n",
    "<b>Returns:</b>\n",
    "   The training data containing the best performing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_feature_selection(X_train, y_train, max_depth, min_samples_leaf):\n",
    "    \"\"\" \n",
    "    This function iteratively removes features from the original training set \n",
    "    until the accuracy no longer improves by more than 1%.\n",
    "    \"\"\"\n",
    "    \n",
    "    best_training_features = X_train  # keeps track of the best performing features\n",
    "    total_num_features = X_train.shape[1] # start with 10 features\n",
    "    last_acc_score = None  # keeps track of accuracy scores\n",
    "   \n",
    "    \n",
    "    for num_features in reversed(range(1, total_num_features+1)): \n",
    "        \n",
    "        # resample to get new training and validation sets out of our training data\n",
    "        X_train_temp, X_val_temp, y_train_temp, y_val_temp  = train_test_split(X_train, \n",
    "                                                            y_train, test_size=.2, \n",
    "                                                            random_state=1234)\n",
    "        \n",
    "        # train model and get accuracy score\n",
    "        acc, model = train_evaluate_DT_model(X_train_temp, X_val_temp, y_train_temp, \n",
    "                                             y_val_temp, max_depth, min_samples_leaf)\n",
    "        print(\"Accuracy for top {0} features: {1}\".format(num_features, acc))\n",
    "        \n",
    "        # if accuracy improves, save the training data so as to keep track that it contains\n",
    "        # the best performing features so far\n",
    "        if last_acc_score and (acc > last_acc_score): \n",
    "            best_training_features = X_train\n",
    "        \n",
    "        last_acc_score = acc\n",
    "        \n",
    "        # get lowest performing feature from the training data\n",
    "        worst_feature_name = feature_to_drop(model, X_train_temp.columns.values)\n",
    "        \n",
    "        # remove lowest performing feature from training data\n",
    "        X_train = X_train.drop(columns=[worst_feature_name])\n",
    "        \n",
    "    return best_training_features\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below calls the `stepwise_feature_selection()` function to perform feature selection. Note the following:\n",
    "\n",
    "1. We will be working with our original training data `X_train` and  `y_train`. \n",
    "    * We will start by changing `X_train` so that it only contains the top 10 features we discovered above.\n",
    "    * In the function `stepwise_feature_selection()`, we will be splitting `X_train` and `y_train` into training and validation (`X_val` and `y_val`) subsets.\n",
    "2. We will train our model with the best hyperparameter values we discovered above.\n",
    "3. The function `stepwise_feature_selection()` returns a DataFrame consisting of the training data that contains the features that result in the best performing DT model.\n",
    "\n",
    "At the end of this process we will have: \n",
    "1. The optimal hyperparameter configuration. \n",
    "2. The optimal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for top 84 features: 0.7208619000979432\n",
      "Accuracy for top 83 features: 0.7208619000979432\n",
      "Accuracy for top 82 features: 0.7208619000979432\n",
      "Accuracy for top 81 features: 0.7208619000979432\n",
      "Accuracy for top 80 features: 0.7208619000979432\n",
      "Accuracy for top 79 features: 0.7208619000979432\n",
      "Accuracy for top 78 features: 0.7208619000979432\n",
      "Accuracy for top 77 features: 0.7208619000979432\n",
      "Accuracy for top 76 features: 0.7208619000979432\n",
      "Accuracy for top 75 features: 0.7208619000979432\n",
      "Accuracy for top 74 features: 0.7208619000979432\n",
      "Accuracy for top 73 features: 0.7208619000979432\n",
      "Accuracy for top 72 features: 0.7208619000979432\n",
      "Accuracy for top 71 features: 0.7208619000979432\n",
      "Accuracy for top 70 features: 0.7208619000979432\n",
      "Accuracy for top 69 features: 0.7208619000979432\n",
      "Accuracy for top 68 features: 0.7208619000979432\n",
      "Accuracy for top 67 features: 0.7208619000979432\n",
      "Accuracy for top 66 features: 0.7208619000979432\n",
      "Accuracy for top 65 features: 0.7208619000979432\n",
      "Accuracy for top 64 features: 0.7208619000979432\n",
      "Accuracy for top 63 features: 0.7208619000979432\n",
      "Accuracy for top 62 features: 0.7208619000979432\n",
      "Accuracy for top 61 features: 0.7208619000979432\n",
      "Accuracy for top 60 features: 0.7208619000979432\n",
      "Accuracy for top 59 features: 0.7208619000979432\n",
      "Accuracy for top 58 features: 0.7208619000979432\n",
      "Accuracy for top 57 features: 0.7208619000979432\n",
      "Accuracy for top 56 features: 0.7208619000979432\n",
      "Accuracy for top 55 features: 0.7208619000979432\n",
      "Accuracy for top 54 features: 0.7208619000979432\n",
      "Accuracy for top 53 features: 0.7208619000979432\n",
      "Accuracy for top 52 features: 0.7208619000979432\n",
      "Accuracy for top 51 features: 0.7208619000979432\n",
      "Accuracy for top 50 features: 0.7208619000979432\n",
      "Accuracy for top 49 features: 0.7208619000979432\n",
      "Accuracy for top 48 features: 0.7208619000979432\n",
      "Accuracy for top 47 features: 0.7208619000979432\n",
      "Accuracy for top 46 features: 0.7208619000979432\n",
      "Accuracy for top 45 features: 0.7208619000979432\n",
      "Accuracy for top 44 features: 0.7208619000979432\n",
      "Accuracy for top 43 features: 0.7208619000979432\n",
      "Accuracy for top 42 features: 0.7208619000979432\n",
      "Accuracy for top 41 features: 0.7208619000979432\n",
      "Accuracy for top 40 features: 0.7208619000979432\n",
      "Accuracy for top 39 features: 0.7208619000979432\n",
      "Accuracy for top 38 features: 0.7208619000979432\n",
      "Accuracy for top 37 features: 0.7208619000979432\n",
      "Accuracy for top 36 features: 0.7208619000979432\n",
      "Accuracy for top 35 features: 0.7208619000979432\n",
      "Accuracy for top 34 features: 0.7208619000979432\n",
      "Accuracy for top 33 features: 0.7208619000979432\n",
      "Accuracy for top 32 features: 0.7208619000979432\n",
      "Accuracy for top 31 features: 0.7208619000979432\n",
      "Accuracy for top 30 features: 0.7208619000979432\n",
      "Accuracy for top 29 features: 0.7208619000979432\n",
      "Accuracy for top 28 features: 0.7208619000979432\n",
      "Accuracy for top 27 features: 0.7208619000979432\n",
      "Accuracy for top 26 features: 0.7208619000979432\n",
      "Accuracy for top 25 features: 0.7208619000979432\n",
      "Accuracy for top 24 features: 0.7208619000979432\n",
      "Accuracy for top 23 features: 0.7208619000979432\n",
      "Accuracy for top 22 features: 0.7208619000979432\n",
      "Accuracy for top 21 features: 0.7208619000979432\n",
      "Accuracy for top 20 features: 0.7208619000979432\n",
      "Accuracy for top 19 features: 0.7208619000979432\n",
      "Accuracy for top 18 features: 0.7208619000979432\n",
      "Accuracy for top 17 features: 0.7208619000979432\n",
      "Accuracy for top 16 features: 0.7208619000979432\n",
      "Accuracy for top 15 features: 0.7208619000979432\n",
      "Accuracy for top 14 features: 0.7208619000979432\n",
      "Accuracy for top 13 features: 0.7208619000979432\n",
      "Accuracy for top 12 features: 0.7208619000979432\n",
      "Accuracy for top 11 features: 0.7208619000979432\n",
      "Accuracy for top 10 features: 0.7208619000979432\n",
      "Accuracy for top 9 features: 0.7208619000979432\n",
      "Accuracy for top 8 features: 0.7208619000979432\n",
      "Accuracy for top 7 features: 0.7208619000979432\n",
      "Accuracy for top 6 features: 0.7206442485580585\n",
      "Accuracy for top 5 features: 0.7206442485580585\n",
      "Accuracy for top 4 features: 0.7206442485580585\n",
      "Accuracy for top 3 features: 0.7226031124170204\n",
      "Accuracy for top 2 features: 0.7171618239199042\n",
      "Accuracy for top 1 features: 0.7171618239199042\n"
     ]
    }
   ],
   "source": [
    "best_md = best_params['max_depth']\n",
    "best_msl = best_params['min_samples_leaf']                   \n",
    "\n",
    "best_performing_features = stepwise_feature_selection(X_train, y_train,\n",
    "                                                      best_md, best_msl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below outputs the best performing features given the heuristics that we decided upon for choosing the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing training data (best features):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['MonthlyMinutes', 'MonthsInService', 'CurrentEquipmentDays'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nBest Performing training data (best features):\\n\")\n",
    "best_performing_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Fit the Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the hyperparameter configuration and features that produce the best decision tree model, we can fit a `DecisionTreeClassifier` with those values. \n",
    "\n",
    "The code cell below fits *one* decision tree classifier using the best hyperparameters and features identified, tests the model on the test set (`X_test`), and obtains the final accuracy score of the model's class label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.7149853085210578\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a DecisionTreeClassifier model object\n",
    "model = DecisionTreeClassifier(max_depth = best_md,\n",
    "                               min_samples_leaf = best_msl)\n",
    "    \n",
    "# 2. Fit the model to the training data \n",
    "model.fit(best_performing_features, y_train)\n",
    "\n",
    "\n",
    "# 3. Make predictions on the test data \n",
    "#(Appropriately, we are only using the features we trained on to test our model)\n",
    "X_test = X_test[['MonthlyMinutes', 'MonthsInService', 'CurrentEquipmentDays']]\n",
    "class_label_predictions = model.predict(X_test)\n",
    "\n",
    "# 4. Compute the accuracy \n",
    "acc_score = accuracy_score(y_test, class_label_predictions)\n",
    "\n",
    "print('Accuracy score: {0}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
